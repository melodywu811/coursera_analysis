{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genetic-begin",
   "metadata": {},
   "source": [
    "# 1.1 Coursera Course List Scraper\n",
    "This notebook presents the codes used to scrape a list of all data science courses on Coursera. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-italy",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time \n",
    "\n",
    "# other libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://towardsdatascience.com/web-scraping-using-selenium-python-8a60f4cf40ab\n",
    "# reference: https://nbviewer.jupyter.org/github/thefirebanks/CourseraScraper/blob/master/coursera_url_scrapper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('../capstone_others/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.coursera.org/browse/data-science?facets=entityTypeTag%3ACourses%2CcategoryMultiTag%3Adata-science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are hidden information in the html that only shows up at scrolling through the page\n",
    "# I program the scrapping to scrape as scrolling \n",
    "\n",
    "# the first step to set the scroll to the beginning of the page\n",
    "driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "\n",
    "# using the height of the page to determine how many scrolls we might need \n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "scrolls = last_height // 2300\n",
    "\n",
    "# set the initial parameters for the scroll-n-scrape\n",
    "top = 0\n",
    "bottom = 0 \n",
    "step = 2300\n",
    "links_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a for loop to take elements as scrolling through the page\n",
    "# append the results (list of urls to a list)\n",
    "\n",
    "for i in range(scrolls):\n",
    "    # print(i, top, bottom) # the print function here is just to make sure the for-loop is going \n",
    "    \n",
    "    # scroll the page to update the results \n",
    "    driver.execute_script(f\"window.scrollTo({top}, {bottom})\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # get the results \n",
    "    results = driver.find_elements_by_xpath('//*[@id=\"rendered-content\"]/div/div/div[1]/section/div[3]/section//a')\n",
    "    links = [result.get_attribute(\"href\") for result in results if \"learn/\" in result.get_attribute(\"href\")]\n",
    "    links_lists.append(links)\n",
    "    \n",
    "    # update the parameters \n",
    "    top = bottom\n",
    "    bottom = bottom + step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the urls and save them in a new list\n",
    "new_list = []\n",
    "\n",
    "for links in links_lists: \n",
    "    for link in links:\n",
    "        new_list.append(link)\n",
    "\n",
    "# removing duplicate items and save as a list\n",
    "final_list = list(set(new_list)) \n",
    "# convert the list into a Dataframe\n",
    "course_urls = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the urls in a DataFrame and export as a csv. \n",
    "# course_urls.to_csv('../data/ds_course_urls.csv') # uncomment to save a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
